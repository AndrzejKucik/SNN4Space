{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "STRATEGY = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "NUM_DEVICES = STRATEGY.num_replicas_in_sync\n",
    "\n",
    "from notebook_utils import create_model, calculate_flops, extract_dataset, extract_ucm, compute_connections, compute_neurons\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) - Creating a model that can be transferred to a spiking model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiking models hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=0.1 #Scale value\n",
    "synapse=0.01#Synapse\n",
    "timesteps=10 #Number of timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_used=\"UCM\" # Accepted: UCM, EuroSAT\n",
    "dataset_dict = {\"UCM\" : \"../dataset/UCM\", \"EuroSAT\" : \"../dataset/EuroSAT\"}\n",
    "img_dict = {\"UCM\" : (256, 256,3), \"EuroSAT\" : (64,64,3)}\n",
    "num_classes_dict = {\"UCM\" : 21, \"EuroSAT\" : 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=dataset_dict[dataset_used]\n",
    "train_percentage=0.70\n",
    "test_percentage=0.2\n",
    "img_shape=img_dict[dataset_used]\n",
    "n_test=1# Number of images to use (Use -1 to use all.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) - Creating a model that can be transferred to a spiking model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg16\" #Model architecture (supported: vgg16 / alexnet)\n",
    "weights_path= \"C:/Users/meoni/Documents/ESA/ACT/SNN/nengo_space_git/Nengo_Space/notebook/models/model.h5\"\n",
    "model, input_layer, output_layer, global_pool = create_model(STRATEGY, model_name=model_name, input_shape=img_shape, num_classes=num_classes_dict[dataset_used], weights_path=weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) - Estimating the model flops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) - Converting model to nengo_dl model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block1_conv1.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E331357240> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block1_conv2.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E331342588> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block2_conv1.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E331284898> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block2_conv2.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E331329908> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block3_conv1.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E33131C6A0> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block3_conv2.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E331335B38> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block3_conv3.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E33125C5F8> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block4_conv1.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E33133ABA8> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block4_conv2.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E33139B9B0> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block4_conv3.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E3314D2048> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block5_conv1.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E3314D2BA8> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block5_conv2.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E3314DDD30> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n",
      "C:\\Users\\meoni\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py:309: UserWarning: block5_conv3.kernel_regularizer has value <tensorflow.python.keras.regularizers.L2 object at 0x000001E3314EF5F8> != None, which is not supported (unless inference_only=True). Falling back to TensorNode.\n",
      "  % (error_msg + \". \" if error_msg else \"\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensors are unhashable. (KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"))Instead, use tensor.ref() as the key.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cd2201fcf7ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                \u001b[0mscale_firing_rates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                \u001b[0msynapse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynapse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                swap_activations={tf.nn.relu: nengo.LIF()})\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnetwork_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, allow_fallback, inference_only, max_to_avg_pool, split_shared_weights, swap_activations, scale_firing_rates, synapse)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;31m# convert model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;31m# set model from the converter in case the converter has changed the model type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    865\u001b[0m                 \u001b[0minput_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_node_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                 net.inputs[input] = net.layer_map[input_layer][input_node_id][\n\u001b[1;32m--> 867\u001b[1;33m                     \u001b[0minput_tensor_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m                 ]\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\nengo_dl\\converter.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nengo_space\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     raise TypeError('Tensors are unhashable. (%s)'\n\u001b[1;32m--> 261\u001b[1;33m                     'Instead, use tensor.ref() as the key.' % self)\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m   \u001b[1;31m# Note: This enables the KerasTensor's overloaded \"right\" binary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Tensors are unhashable. (KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"))Instead, use tensor.ref() as the key."
     ]
    }
   ],
   "source": [
    "# Convert to a Nengo network\n",
    "converter = nengo_dl.Converter(model,\n",
    "                               scale_firing_rates=scale,\n",
    "                               synapse=synapse,\n",
    "                               swap_activations={tf.nn.relu: nengo.LIF()})\n",
    "\n",
    "network_input = converter.inputs[input_layer]\n",
    "network_output = converter.outputs[output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding probes to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = len(converter.layers)#Number of layers\n",
    "probe_layer = []\n",
    "with converter.net:\n",
    "    for layer in model.layers:\n",
    "        probe_layer.append(nengo.Probe(converter.layers[layer]))\n",
    "    nengo_dl.configure_settings(stateful=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_used == \"UCM\":\n",
    "    import tensorflow_datasets as tfds\n",
    "    [_, _, _, _, test_data, test_label] = extract_ucm(dataset_path, [img_shape[0],img_shape[1]], train_percentage, 1-(train_percentage+test_percentage))\n",
    "    test_zeros =  np.zeros([len(test_data), test_data[0].shape[0], test_data[1].shape[1], test_data[2].shape[2]])\n",
    "\n",
    "    for n in range(len(test_data)):\n",
    "        test_zeros[n] = test_data[n]\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_zeros, test_label))\n",
    "    \n",
    "else:\n",
    "    [_, _, test_ds] = extract_dataset(dataset_path=dataset_path, train_percentage=train_percentage, test_percentage=test_percentage, batch_size=1, img_size=(img_shape[0], img_shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing number of test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_test > -1:\n",
    "    test_ds = test_ds.take(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting samples and shaping according to the number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [(n[0].numpy(), n[1].numpy()) for n in test_ds.take(n_test)]\n",
    "x_test = np.array([n[0] for n in test_data])\n",
    "y_test = np.array([n[1] for n in test_data])\n",
    "\n",
    "# Tile images according to the number of timesteps\n",
    "tiled_test_images = np.tile(np.reshape(x_test, (x_test.shape[0], 1, -1)), (1, timesteps, 1))\n",
    "test_labels = y_test.reshape((y_test.shape[0], 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(converter.net) as sim:\n",
    "    # Record how much time it takes\n",
    "    start = time()\n",
    "    data = sim.predict({network_input: tiled_test_images})\n",
    "    print('Time to make a prediction with {} timestep(s): {}.'.format(timesteps, timedelta(seconds=time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_estimate_layer(model, layer_idx, spikes_measurements=None, probe_layers=None, dt=0.001, n_timesteps=100, spiking_model=True, device=\"cpu\", if_neurons=False, verbose=False):\n",
    "    devices = {\n",
    "        # https://ieeexplore.ieee.org/abstract/document/7054508\n",
    "        # TODO: CPU neuron energy depends on neuron type\n",
    "        \"cpu\": dict(spiking=False, energy_per_synop=8.6e-9, energy_per_neuron=8.6e-9),\n",
    "        \"gpu\": dict(spiking=False, energy_per_synop=0.3e-9, energy_per_neuron=0.3e-9),\n",
    "        \"arm\": dict(spiking=False, energy_per_synop=0.9e-9, energy_per_neuron=0.9e-9),\n",
    "        # https://www.researchgate.net/publication/322548911_Loihi_A_Neuromorphic_Manycore_Processor_with_On-Chip_Learning\n",
    "        \"loihi\": dict(\n",
    "            spiking=True,\n",
    "            energy_per_synop=(23.6 + 3.5) * 1e-12,\n",
    "            energy_per_neuron=81e-12,\n",
    "        ),\n",
    "        # https://arxiv.org/abs/1903.08941\n",
    "        \"spinnaker\": dict(\n",
    "            spiking=True, energy_per_synop=13.3e-9, energy_per_neuron=26e-9\n",
    "        ),\n",
    "        \"spinnaker2\": dict(\n",
    "            spiking=True, energy_per_synop=450e-12, energy_per_neuron=2.19e-9\n",
    "        ),\n",
    "    }\n",
    "    energy_dict = devices[device]\n",
    "    if (spiking_model == True) and (energy_dict[\"spiking\"] == False):\n",
    "        print(\"Error! Impossible to infer Spiking models on standard hardware!\")\n",
    "        return -1\n",
    "    \n",
    "    energy_per_synop = energy_dict[\"energy_per_synop\"]\n",
    "    energy_per_neuron = energy_dict[\"energy_per_neuron\"]\n",
    "    n_neurons = compute_neurons(model.layers[layer_idx]) \n",
    "    n_connections_per_neuron = compute_connections(model.layers[layer_idx])\n",
    "    \n",
    "    if (spiking_model == False):\n",
    "        f_in = 1/dt\n",
    "        n_timesteps = 1\n",
    "        f_out = 1 / dt\n",
    "    else:\n",
    "        spikes_out = spikes_measurements[probe_layers[layer_idx]] \n",
    "        if layer_idx == 0:\n",
    "            f_in = 0\n",
    "        else:\n",
    "            spikes_in = spikes_measurements[probe_layers[layer_idx - 1]]\n",
    "            f_in = np.sum(spikes_in)/ (spikes_in.shape[1] * spikes_in.shape[2] * dt)\n",
    "        \n",
    "        f_out = np.sum(spikes_out)/ (spikes_out.shape[1] * spikes_out.shape[2] * dt)\n",
    "    \n",
    "    print(n_timesteps)\n",
    "    #synop_energy/inference = energy/op * ops/event * events/s * s/timestep * timesteps/inference\n",
    "    synop_energy = energy_per_synop * n_connections_per_neuron * n_neurons * f_in * dt * n_timesteps\n",
    "    if if_neurons: #neurons are update only when an output spike is produced\n",
    "        #neuron_energy/inference = energy/op * ops/event * events/s * s/timestep * timesteps/inference\n",
    "        neuron_energy = energy_per_neuron * n_neurons * f_out * dt * n_timesteps\n",
    "    else: #neurons are update every timestep (for instance, to implement alpha functions)\n",
    "        #neuron_energy/inference = energy/op * ops/timestep * timesteps/inference\n",
    "        neuron_energy = energy_per_neuron * n_neurons * n_timesteps\n",
    "    if verbose:\n",
    "        print(\"--- Layer: \", model.layers[layer_idx].name, \" ---\")\n",
    "        print(\"\\tSynop energy: \", synop_energy, \"J/inference\")\n",
    "        print(\"\\tNeuron energy: \", neuron_energy, \"J/inference\")\n",
    "        print(\"\\tTotal energy: \", neuron_energy+synop_energy, \"J/inference\")\n",
    "    return neuron_energy,synop_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------- ANN model ---------\")\n",
    "neuron_energy,synop_energy = 0,0\n",
    "for layer_idx in range(len(model.layers)):\n",
    "    neuron_energy_layer,synop_energy_layer=energy_estimate_layer(model, layer_idx, spikes_measurements=data, probe_layers=probe_layer, dt=sim.dt, n_timesteps=timesteps, spiking_model=False, device=\"loihi\", verbose=False)\n",
    "    neuron_energy+=neuron_energy_layer\n",
    "    synop_energy+=synop_energy_layer\n",
    "print(\"--------- Total energy ---------\")\n",
    "print(\"Synop energy: \", synop_energy, \"J/inference\")\n",
    "print(\"Neuron energy: \", neuron_energy, \"J/inference\")\n",
    "print(\"Total energy: \", synop_energy+neuron_energy,\" J/inference\")\n",
    "\n",
    "print(\"--------- SNN model ---------\")\n",
    "neuron_energy_layer,synop_energy_layer = 0,0\n",
    "for layer_idx in range(len(model.layers)):\n",
    "    neuron_energy_layer,synop_energy_layer=energy_estimate_layer(model, layer_idx, spikes_measurements=data, probe_layers=probe_layer, dt=sim.dt, n_timesteps=10, spiking_model=True, device=\"loihi\", verbose=False)\n",
    "    neuron_energy+=neuron_energy_layer\n",
    "    synop_energy+=synop_energy_layer\n",
    "print(\"--------- Total energy ---------\")\n",
    "print(\"Synop energy: \", synop_energy, \"J/inference\")\n",
    "print(\"Neuron energy: \", neuron_energy, \"J/inference\")\n",
    "print(\"Total energy: \", synop_energy+neuron_energy,\" J/inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
